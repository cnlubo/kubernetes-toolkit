# 06-4.部署scheduler集群

该集群包含3个节点,启动后将通过竞争选举机制产生一个 leader节点,其它节点为阻塞状态.当leader 节点不可用后,剩余节点将再次进行选举产生新的leader节点,从而保证服务的可用性.

为保证通信安全,先生成x509证书和私钥,kube-scheduler在如下两种情况下使用该证书

- 与kube-apiserver的安全端口通信
- 在安全端口(https,10251)输出prometheus格式的 metrics

## 准备工作
下载最新版本的二进制文件,安装和配置flanneld
参考:
1. 06-0.master节点部署
2. 05.flannel网络部署

## 创建分发证书和私钥
脚本:06.10.scheduler-cert.sh

创建证书签名请求：

```bash
[ ! -d /opt/k8s/certs ] && mkdir -p /opt/k8s/certs
[ -d /opt/k8s/certs/kube-scheduler ] && rm -rf /opt/k8s/certs/kube-scheduler
mkdir -p /opt/k8s/certs/kube-scheduler
cd /opt/k8s/certs/kube-scheduler || exit
info "create kube-scheduler-csr.json ....."
source /opt/k8s/bin/environment.sh
cat > kube-scheduler-csr.json <<EOF
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
      "$MASTER_1",
      "$MASTER_2",
      "$MASTER_3"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-scheduler",
        "OU": "k8s"
      }
    ]
}
EOF

```
- hosts列表包含所有kube-scheduler节点 IP
- CN为system:kube-scheduler,O为system:kube-scheduler,kubernetes内置的ClusterRoleBindings system:kube-scheduler 将赋予kube-scheduler工作所需的权限

## kube-scheduler 初始化
脚本:06.11.scheduler-config.sh

### kubeconfig文件

kube-scheduler使用kubeconfig文件访问apiserver,该文件提供了apiserver地址、嵌入的CA证书和kube-scheduler证书

```bash
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig

# 分发 kubeconfig 到所有 master 节点
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp kube-scheduler.kubeconfig $k8s_user@${node_ip}:/etc/kubernetes/
  done
```
### 创建分发kube-scheduler配置文件

```bash
source /opt/k8s/bin/environment.sh
cat >kube-scheduler.yaml.template <<EOF
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
bindTimeoutSeconds: 600
clientConnection:
  burst: 200
  kubeconfig: "/etc/kubernetes/kube-scheduler.kubeconfig"
  qps: 100
enableContentionProfiling: false
enableProfiling: true
hardPodAffinitySymmetricWeight: 1
healthzBindAddress: ##NODE_IP##:10251
leaderElection:
  leaderElect: true
metricsBindAddress: ##NODE_IP##:10251
EOF
```
- \--kubeconfig：指定kubeconfig文件路径,kube-scheduler使用它连接和验证kube-apiserver
- \--leader-elect=true:集群运行模式,启用选举功能.被选为leader的节点负责处理工作,其它节点为阻塞状态.

```bash
# deploy kube-scheduler配置文件到所有master节点
node_counts=${#MASTER_IPS[@]}
for (( i=0; i < $node_counts; i++ ))
do
    sed -e "s/##NODE_IP##/${MASTER_IPS[i]}/" kube-scheduler.yaml.template > kube-scheduler-${MASTER_IPS[i]}.yaml
done

for node_ip in "${MASTER_IPS[@]}"
do
    echo ">>> ${node_ip}"
    scp kube-scheduler-${node_ip}.yaml ${k8s_user:?}@${node_ip}:/etc/kubernetes/kube-scheduler.yaml
done
```

### 创建分发 systemd unit文件

```bash
[ ! -d /opt/k8s/services ] && mkdir -p /opt/k8s/services
[ -d /opt/k8s/services/kube-scheduler ] && rm -rf /opt/k8s/services/kube-scheduler
mkdir -p /opt/k8s/services/kube-scheduler
cd /opt/k8s/services/kube-scheduler || return
cat > kube-scheduler.service.template <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
WorkingDirectory=${K8S_DIR}/kube-scheduler
ExecStart=/opt/k8s/bin/kube-scheduler \\
  --config=/etc/kubernetes/kube-scheduler.yaml \\
  --bind-address=##NODE_IP## \\
  --secure-port=10259 \\
  --port=0 \\
  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-allowed-names="" \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/aggregator-ca.pem \\
  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=${K8S_DIR}/logs/kube-scheduler \\
  --v=2
Restart=on-failure
RestartSec=5
User=$k8s_user

[Install]
WantedBy=multi-user.target
EOF
```
- \--leader-elect=true:集群运行模式,启用选举功能,被选为leader的节点负责处理工作,其它节点为阻塞状态
- User=ak47 使用ak47账户运行

## 检查服务运行状态

```bash
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh $k8s_user@${node_ip} "sudo systemctl status kube-scheduler|grep Active"
  done
# 确保状态为 active (running)否则查看日志
sudo journalctl -u kube-scheduler
```
## 查看输出的 metric

==注意:==
以下命令在kube-scheduler节点上执行。
kube-scheduler监听10251端口,接收http请求
```
sudo netstat -lnpt|grep kube-sche
curl -s http://127.0.0.1:10251/metrics | head
```
## 集群的高可用

停掉一个或两个节点上的kube-scheduler服务，看其它节点是否获取了leader权限

```bash
# 查看当前的 leader
kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
```
## 检查集群信息
```bash
kubectl cluster-info
# Kubernetes master is running at https://192.168.3.253:8443
kubectl get all --all-namespaces
kubectl get componentstatuses
```
